{% comment %}
# This file is part of OpenHatch.
# Copyright (C) 2010 OpenHatch Inc.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
{% endcomment %}
<h1>Some tables</h1>


<h2>Diagnostics for bug tracker updating</h2>

<h3>What are we diagnosing?</h3>

<p>So we store these "Bug" objects. How do they update?</p>

<p>Each Bug object doesn't know how to update itself. Instead, we have
a nightly job per bug tracker. When that job wakes up,
it refreshes all the bugs that came from that bug tracker.
</p>

<p>So if a Bug object's last_polled stamp is more old than one day + one hour,
then it means we're not properly updating it from its bug tracker. If it's older
than <em>two</em> days, then something is terribly wrong!</p>

<h3>Data</h3>

<table>
{% for key, value in bug_diagnostics.items %}
<tr>
<td>{{ key }}:</td>
<td>{{ value }}</td>
</tr>
{% endfor %}
</table>

<h2>Diagnostics for Data Import Attempts</h2>

<h3>What are we diagnosing?</h3>

<p>These diagnostics explain how well we are handling "DIAs".
A DataImportAttempt is created whenever a user submits a username
or email address to the profile importer. When that happens, we create
a DataImportAttempt object and save it to the database. We also enqueue
a job to the background job daemon (celeryd).
</p>

<p>
If we're lucky, the background jobs get handled quickly. The imports
from Ohloh can take <em>minutes</em>, whereas faster services like
Bitbucket and Github respond within seconds.
</p>

<p>We run some number (typically 4) of celeryd workers to execute
these jobs. That means can run that many (typically 4) tasks at once,
or so. If users are submitting jobs faster than we're processing them,
then their experience will suck.</p>

<p>Really, at any given time, the goal is 0 uncompleted DataImportAttempts.
As <em>soon</em> as someone hits submit to start the importer, there will
be a few uncompleted DIAs. Hopefully we'll finish them within one minute.
If we take more than 5 minutes, then something is <em>definitely</em> wrong!</p>

<h3>Data</h3>

<table>
{% for key, value in dia_diagnostics.items %}
<tr>
<td>{{ key }}:</td>
<td>{{ value }}</td>
</tr>
{% endfor %}
</table>

